{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924c3083",
   "metadata": {},
   "source": [
    "\n",
    "# Glassdoor Review Scraper WIP\n",
    "\n",
    "## How to Use\n",
    "Designed to be agnostic of which company you are scraping all you need to do is the following:\n",
    "\n",
    " 1. Ensure this code is loaded in a local IDE (Built and tested in VSCode), cloud based IDE's like Google Collab won't work due the requirement for an installed Chrome instance\n",
    " 2. Ensure you have chrome installed on your device\n",
    " 3. Run the following blocks of code\n",
    " 4. When prompted copy in the url to the reviews page of the company who's reviews you want to scrape\n",
    "\n",
    "\t> Example url: https://www.glassdoor.co.uk/Reviews/eBay-Reviews-E7853.htm\n",
    "\t\n",
    " 5. Entering a glassdoor username and password is optional however the tool will only be able to return the first page of reviews without them, to mitigate risk they are only held in memory long enough to pass to Glassdoor\n",
    " 6. How many pages of reviews you wish to return, each page holds roughly 10 reviews\n",
    "\n",
    "## Current Issues\n",
    "\n",
    " - Can't be run in headless mode due to the Captcha solving method, currently exploring alternative methods to get around this requirement\n",
    " - No unit tests, these will be included before final submission I've just not had time to include them before the formative!\n",
    " - Occasionally when signing in it will click the forgot password link instead of the sign in button causing the code to fail, currently looking into a solution\n",
    " - No logic to check you're asking for more pages than there are pages for that company, the issue is there's no clear identifier for the page count as they all use the same class and identifiers. solvable problem just not have time to implement before formative\n",
    " - Code is an uncommented mess and needs serious clean up and commenting before final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q selenium pandas bs4 seleniumbase tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from seleniumbase import Driver\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import zip_longest\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eda742",
   "metadata": {},
   "source": [
    "The following will likely be integrated into the login function before final submission\n",
    "\n",
    "If using VSCode the search bar at the top of the screen will be where you're prompted to fill in details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input(\"Enter the Glassdoor reviews URL: \")\n",
    "email = input(\"Enter your Glassdoor email: \")\n",
    "password = input(\"Enter your Glassdoor password: \")\n",
    "page_count = int(input(\"Enter how many pages of results to trawl through: \")) #Each Page has 10 reviews\n",
    "df_reviews = pd.DataFrame(columns=[\"Title\",\"Rating\",\"Date\",\"Job Title\",\"Pros\",\"Cons\"])\n",
    "\n",
    "if email and password:\n",
    "    page_count = page_count\n",
    "else:\n",
    "    page_count = 1\n",
    "    print(\"Only returning first page as no login details were provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b3a04",
   "metadata": {},
   "source": [
    "The Following is the main scraping tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlassdoorScraper:\n",
    "    def __init__(self, url, email=\"\", password=\"\", page_count=1):\n",
    "        self.url = url\n",
    "        self.email = email\n",
    "        self.password = password\n",
    "        self.page_count = page_count\n",
    "        self.driver = Driver(uc=True)\n",
    "        #self.driver.set_window_position(0,-2000)\n",
    "        del email, password\n",
    "        print(\"Initialized GlassdoorScraper\")\n",
    "\n",
    "    def login(self):\n",
    "        self.driver.uc_open_with_reconnect(self.url, reconnect_time=6)\n",
    "        self.driver.uc_gui_click_captcha()\n",
    "        try:\n",
    "            if self.email and self.password:\n",
    "                self.driver.click('button[aria-label=\"sign in\"]', timeout=5)\n",
    "                self.driver.type('input[type=\"email\"]', self.email, timeout=2)\n",
    "                self.driver.click('button[data-test=\"continue-with-email-modal\"]', timeout=2)\n",
    "                self.driver.sleep(2)\n",
    "                self.driver.type('input[type=\"password\"]', self.password, timeout=2)\n",
    "                self.driver.click('button[class=\"Button Button\"]', timeout=2)\n",
    "                self.driver.click('button[id=\"onetrust-accept-btn-handler\"]', timeout=2)\n",
    "                print(\"Logged in successfully\")\n",
    "            else:\n",
    "                self.driver.click('button[id=\"onetrust-accept-btn-handler\"]', timeout=2)\n",
    "                print(\"Skipping sign-in: email or password empty\")\n",
    "        except NameError:\n",
    "            print(\"Login elements not found, closing scraper\")\n",
    "            self.driver.quit()\n",
    "        del self.email, self.password\n",
    "        time.sleep(5)\n",
    "\n",
    "    def grab_reviews(self):\n",
    "        \n",
    "        rows = []\n",
    "        reviews = self.driver.find_element(by=By.ID,value=\"ReviewsFeed\")\n",
    "        soup = BeautifulSoup(reviews.get_attribute('innerHTML'),'html.parser')\n",
    "        pros = [a.get_text(separator=\" \", strip=True) for a in soup.find_all(attrs={\"data-test\": \"review-text-PROS\"})]\n",
    "        cons = [a.get_text(separator=\" \", strip=True) for a in soup.find_all(attrs={\"data-test\": \"review-text-CONS\"})]\n",
    "        title = [a.get_text(separator=\" \", strip=True) for a in soup.find_all(attrs={\"data-test\": \"review-details-title\"})]\n",
    "        job = [a.get_text(separator=\" \", strip=True) for a in soup.find_all(attrs={\"data-test\": \"review-avatar-label\"})]\n",
    "        rating = [a.get_text(separator=\" \", strip=True) for a in soup.find_all(attrs={\"data-test\": \"review-rating-label\"})]\n",
    "        date = [a.get_text(separator=\" \", strip=True) for a in soup.find_all(class_ = \"timestamp_reviewDate__dsF9n\")]\n",
    "\n",
    "        for p, c, t, j, r, d, in zip(pros, cons, title, job, rating, date):\n",
    "            rows.append({\"Title\": t,\"Rating\": r,\"Date\": d, \"Job Title\": j, \"Pros\": p, \"Cons\": c})\n",
    "        return rows\n",
    "\n",
    "\n",
    "    def scrape_reviews(self):\n",
    "        print(\"Starting to scrape reviews\")\n",
    "        \n",
    "        df_reviews = pd.DataFrame(GlassdoorScraper.grab_reviews(self))\n",
    "\n",
    "        for page in tqdm(range(1, page_count), desc=\"Scraping Pages\", unit=\"page\"):\n",
    "            ActionChains(self.driver).move_to_element(self.driver.find_element(By.CLASS_NAME, value=\"PaginationContainer_paginationContainer__bDHGx\")).perform()\n",
    "            self.driver.click('button[data-test=\"next-page\"]', timeout=2)\n",
    "            self.driver.sleep(2)\n",
    "            ActionChains(self.driver).move_to_element(self.driver.find_element(by=By.ID,value=\"ReviewsFeed\")).perform()\n",
    "            GlassdoorScraper.grab_reviews(self)\n",
    "            df_reviews_cont = pd.DataFrame(GlassdoorScraper.grab_reviews(self))\n",
    "            df_reviews = pd.concat([df_reviews, df_reviews_cont], ignore_index=True)\n",
    "        \n",
    "        self.driver.quit()\n",
    "        clear_output(wait=True)\n",
    "        print(\"Scraping complete. Dataset Info: \\n\")\n",
    "        print(df_reviews.info())\n",
    "        return df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d33b31",
   "metadata": {},
   "source": [
    "Use the following to call the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baec0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = GlassdoorScraper(url, email, password, page_count)\n",
    "scraper.login()\n",
    "df_reviews = scraper.scrape_reviews()\n",
    "df_reviews.to_csv(\"glassdoor_reviews.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
